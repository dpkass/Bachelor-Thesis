\chapter{Exploratory}

\begin{markdown}
## Experiment Setup
- **Computational Environment:**
  - List software and libraries used (Python version, NumPy, SciPy, etc.).
- **Algorithms Overview:**
  - **DP (Optimal):** Used as a benchmark for comparing other algorithms.
  - **Greedy Algorithm:** Simple heuristic; Makes decision that minimally increases weights.
    - Turns out it just rotates the machines, i.e. 1->2->3->1 etc.
    - Therefore it simply Balances the amount of jobs per machine.
  - **Balanced Sequential Insert (BSI):** Aims to balance workload across machines.
  - **k-Lookahead Algorithm:** Incorporates future job information into current decisions.
    - Uses DP to search k steps ahead
    - Then does one 1 st, i.e. inserts one weight
    - Time duration for k=15 is approx. equal to DP for small m
  - **Heavy First:** Sort by weight then insert heaviest first following greedy
    strategy.
    - Greedy means simply insert into each machine, evaluate and choose the machine which least increase the target function
- **Sort & Split:** Sort by weight, then split into $m$ equally sized arrays, then
  sort each array by precendence. (Predecessor of Balanced Sequential Insert)
- **Instance Generators:**
  - **List of Generators:**
    - Increasing weights.
    - Decreasing weights.
    - Small random weights. (1-100)
    - Small span large weights. (10000-10100)
    - Large span large weights. (1-10000)
    - Low then high weights. (first half 1-100, then 900-1000)
    - High then low weights. (reversed)
    - Large span random non-increasing/non-decreasing weights. (equivalent to increasing
      decreasing)
  - **Rationale:**
    - To assess algorithm performance across diverse scenarios.
    - Understand how weight distributions affect scheduling.
- **Parameters:**
  - Number of jobs $n = 150$.
  - Number of machines $m \in \{2, 3, 4\}$.
  - Seeds for random number generation to ensure reproducibility.
- **Performance Metrics:**
  - **Relative Performance Ratio (Quality):** Algorithm's total weighted completion time
    divided
    by the optimal value.
  - **Softmax Scaled Performance:** Normalized performance scores.
  - **Variation Coefficient:** Measures stability over different seeds.
  - **Relative Improvement:** Improvement relative to increasing $m$. (only for $m$ variation)
  - ==**Relative Time Improvement:** Change in computational time with varying $m$.== #todo 
- **Data Collection:**
  - Explain how results are stored (tables, data arrays).
  - Describe any preprocessing or data transformation steps.

## Algorithm Definitions
- **Detailed Descriptions:**
  - **Greedy:**
    - **Logic:** Assign jobs to machines based on a priority rule (e.g., earliest finish
      time).
    - **Steps:**
      - Initialize machine completion times.
      - For each job, select the machine with the earliest availability.
    - **Intuition:** Fast and simple but may not always yield optimal results.
  - **Balanced Sequential Insert:**
    - **Logic:** Distribute jobs to balance the total weighted completion times across
      machines.
    - **Steps:**
      - Sort jobs in non-increasing order of weights.
      - Sequentially assign jobs to machines, ensuring balance.
    - **Intuition:** Aims to minimize imbalance, potentially reducing total completion time.
  - **k-Lookahead:**
    - **Logic:** Similar to greedy but considers the next $k$ jobs before making a decision.
    - **Parameters:** Define $k$ (lookahead window size).
    - **Intuition:** By looking ahead, it may make better immediate decisions.
  - **DP (Optimal):**
    - Reiterate that it's used as a baseline; refer back to the detailed description in
      Chapter 2.
  - **Include Pseudocode:**
    - Provide pseudocode or flowcharts for each algorithm to aid understanding.
- **Expected Performance:**
  - Discuss theoretical strengths and weaknesses.
  - Hypothesize on which instance types each algorithm may perform well or poorly.

## Observation
- **Presentation of Results:**
  - Use tables to display performance metrics for each algorithm across instance types and machine counts.
  - Include graphs or heatmaps for visual representation.
- **Analysis of Metrics:**
  - **Relative Performance Ratio:**
    - Identify algorithms that consistently perform close to optimal.
    - Highlight any significant deviations.
  - **Variation Coefficient:**
    - Comment on the stability of each algorithm's performance.
    - Discuss high variability cases.
  - **Scalability:**
    - Analyze how algorithms perform as the number of machines increases.
    - Observe trends in relative improvement or degradation.
- **Instance Type Insights:**
  - For each instance generator:
    - Discuss which algorithms perform best and why.
    - Note any patterns related to weight distributions.
  - Examples:
    - **Decreasing Weights:** Algorithms may perform optimally due to predictable order.
    - **Random Weights:** Performance may vary; discuss observed trends.
- **Comparative Analysis:**
  - Compare select algorithms against each other.
  - Identify scenarios where one algorithm outperforms others.
- **Unexpected Findings:**
  - Highlight any surprising results (e.g., an algorithm performing poorly on an expected instance type).
    - BSI getting faster with more machines (surprising on first glance)
  - Speculate on reasons for these observations.

## Conclusions
- **Summary of Key Findings:**
  - Recap which algorithms are most effective overall.
  - Discuss the trade-offs between computational time and solution quality.
- **Practical Implications:**
  - Provide recommendations for algorithm selection based on instance characteristics.
  - Suggest when it may be acceptable to use heuristics over optimal solutions.
- **Reflection:**
  - Connect results back to the initial motivation.
  - Emphasize the importance of considering instance types in algorithm performance.
\end{markdown}


\section{Experiment Setup}

\section{Algorithm Definitions} \label{sec:algorithm_definitions}

\section{Observations}

\section{Conclusion}
