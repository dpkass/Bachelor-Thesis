\section{Experiment Setup} \label{sec:experiment_setup}

In this section, we describe the experimental setup used to evaluate the performance of various scheduling algorithms under local precedence constraints. The setup encompasses the computational environment, the algorithms implemented, the instance generators for creating test cases, the parameters considered, the performance metrics employed, and the data collection methods.

We evaluated six algorithms designed to address the scheduling problem with local precedence constraints in distinct ways. These include:

\begin{itemize}
    \item \textbf{Dynamic Programming (DP) Algorithm}: Serves as the optimal solution and benchmark for comparing other algorithms.
    \item \textbf{Least Loaded}: A simple heuristic that assigns jobs sequentially to the machine that has the lowest sum of weighted completion times.
    \item \textbf{Heavy First}: Sorts jobs by weight in non-increasing order and assigns the heaviest jobs first, following a greedy strategy.
    \item \textbf{$k$-Lookahead Algorithm}: Incorporates future job information by considering the next $k$ jobs before making a scheduling decision.
    \item \textbf{Sort \& Split}: Sorts jobs by weight and splits them into $m$ equally sized arrays, then assigns each array to a machine while maintaining local precedence.
    \item \textbf{Balanced Sequential Insert (BSI)}: Aims to balance the workload across machines by distributing jobs based on their weights.
\end{itemize}

To assess the algorithms across diverse scenarios, we employed several instance generators that produce different weight distributions. These generators are categorized as follows:


\begin{table}[h]
    \label{tab:instance_generators}
    \begin{tabularx}{\textwidth}{@{}XXp{7cm}@{}}
        \toprule
        \textbf{Generator Name}                    & \textbf{Interval}                      & \textbf{Description}                                                                            \\
        \midrule
        \textbf{Increasing Weights}                & $[1, n]$                               & Generates jobs with weights increasing sequentially in the range $[1, n]$.                      \\
        \textbf{Decreasing Weights}                & $[1, n]$                               & Generates jobs with weights decreasing sequentially in the range $[1, n]$.                      \\
        \midrule
        \textbf{Small Weights}                     & $[1, 100)$                             & Generates jobs with uniformly random weights.                                                   \\
        \textbf{Small Span Large Weights}          & $[100{,}000, 100{,}100)$               & Generates jobs with uniformly random weights.                                                   \\
        \textbf{Large Span Large Weights}          & $[10{,}000, 100{,}000)$                & Generates jobs with uniformly random weights.                                                   \\
        \textbf{Large Span Non-Decreasing Weights} & $[1, 100{,}000)$                       & Generates jobs with uniformly random weights sorted in non-decreasing order.                    \\
        \textbf{Large Span Non-Increasing Weights} & $[1, 100{,}000)$                       & Generates jobs with uniformly random weights sorted in non-increasing order.                    \\
        \textbf{Low Then High Weights}             & $[1, 100) \text{ and } [900, 1{,}000)$ & The first half of the jobs have weights in $[1, 100)$, and the second half in $[900, 1{,}000)$. \\
        \textbf{High Then Low Weights}             & $[900, 1{,}000) \text{ and } [1, 100)$ & The first half of the jobs have weights in $[900, 1{,}000)$, and the second half in $[1, 100)$. \\
        \bottomrule
    \end{tabularx}
    \caption{Instance Generators for Scheduling Algorithms}
\end{table}

These instance generators were chosen to simulate a variety of real-world scenarios and to understand how different weight distributions affect the performance of the scheduling algorithms. By testing the algorithms on both ordered and random weight distributions, we aim to uncover their strengths and weaknesses under varying conditions.

The experiments were conducted using the following parameters:

\begin{itemize}
    \item \textbf{Number of Jobs ($n$)}: $150$.
    \item \textbf{Number of Machines ($m$)}: $2$, $3$, and $4$.
    \item \textbf{Random Seeds}: $0$ to $9$
\end{itemize}

The random seeds ensure reproducibility and allow us to assess performance stability across different random states. To evaluate and compare the algorithms, we employed the following performance metrics:

\begin{itemize}
    \item \textbf{Relative Performance Ratio (Quality)}: The ratio of the algorithm's total weighted completion time to that of the optimal solution.
    \item \textbf{Standard Deviation of RPR}: The standard deviation of the relative performance ratio over different seeds.
    \item \textbf{Relative Improvement}: The improvement in performance relative to increasing the number of machines $m$.
\end{itemize}

These metrics are used to highlight differences between algorithms, measure the stability of each algorithm's performance, and assess whether algorithms perform better or asymptotically approach the optimal solution as the number of machines increases.

Results from the experiments were stored using \texttt{xarray} DataArrays and Datasets, facilitating easy manipulation and analysis of multi-dimensional data. Each computed value was saved in NetCDF (\texttt{.nc}) files for persistence and later retrieval.
